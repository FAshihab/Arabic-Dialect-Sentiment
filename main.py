import pandas as pd
import re
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report

# 1. Text Preprocessing Function | ÙˆØ¸ÙŠÙØ© Ù…Ø¹Ø§Ù„Ø¬Ø© ÙˆØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ
def clean_arabic_text(text):
    text = str(text)
    # Normalize Alef, Ya, and Ta Marbuta | ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ø£Ù„Ù ÙˆØ§Ù„ÙŠØ§Ø¡ ÙˆØ§Ù„ØªØ§Ø¡ Ø§Ù„Ù…Ø±Ø¨ÙˆØ·Ø©
    text = re.sub(r'[Ø¥Ø£Ø¢]', 'Ø§', text)
    text = re.sub(r'Ù‰', 'ÙŠ', text)
    text = re.sub(r'Ø©', 'Ù‡', text)
    # Remove Diacritics (Tashkeel) | Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„
    text = re.sub(r'[\u064B-\u0652]', '', text)
    # Remove non-Arabic characters | Ø¥Ø²Ø§Ù„Ø© Ø£ÙŠ Ø±Ù…ÙˆØ² ØºÙŠØ± Ø¹Ø±Ø¨ÙŠØ©
    text = re.sub(r'[^\u0600-\u06FF\s]', '', text)
    # Remove character elongation | Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø­Ø±ÙˆÙ Ø§Ù„Ù…ÙƒØ±Ø±Ø© (ØªØ·ÙˆÙŠÙ„)
    text = re.sub(r'(.)\1+', r'\1', text)
    return text.strip()

print(" Starting the project... | Ø¬Ø§Ø±ÙŠ Ø¨Ø¯Ø¡ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹...")

# 2. Load or Generate Dataset | ØªØ­Ù…ÙŠÙ„ Ø£Ùˆ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
try:
    # Try to load existing file | Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯
    df = pd.read_csv('data/large_dataset.csv', names=['label', 'text'])
    
    # If file is too small, let's boost it! | Ø¥Ø°Ø§ Ø§Ù„Ù…Ù„Ù ØµØºÙŠØ± Ø¬Ø¯Ø§Ù‹ØŒ Ø®Ù„ÙŠÙ†Ø§ Ù†ÙƒØ¨Ø±Ù‡
    if len(df) < 20:
        print("âš ï¸ Dataset too small, generating synthetic data... | Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù‚Ù„ÙŠÙ„Ø©ØŒ Ø¬Ø§Ø±ÙŠ ØªÙˆÙ„ÙŠØ¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©...")
        # Synthetic Data | Ø¨ÙŠØ§Ù†Ø§Øª Ø§ØµØ·Ù†Ø§Ø¹ÙŠØ© Ù„ØªØ¹Ù„ÙŠÙ… Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„
        extra_data = {
            'text': [
                "Ø§Ù„Ø§ÙƒÙ„ ÙŠØ¬Ù†Ù†", "Ø®Ø¯Ù…Ø© Ø±Ø§Ø¦Ø¹Ø©", "Ù…Ù…ØªØ§Ø² Ø¬Ø¯Ø§", "Ø±Ù‡ÙŠØ¨ Ø§Ù†ØµØ­ÙƒÙ… ÙÙŠÙ‡", "Ø¨Ø·Ù„ Ø¨Ø·Ù„",
                "Ø³ÙŠØ¡ Ø¬Ø¯Ø§", "ØªØ¬Ø±Ø¨Ø© ØªØ¹ÙŠØ³Ø©", "Ù…Ø§ Ø§Ù†ØµØ­ ÙÙŠÙ‡", "Ø§ÙƒÙ„ Ø¨Ø§Ø±Ø¯", "ØªØ§Ø®ÙŠØ± ÙÙŠ Ø§Ù„Ø·Ù„Ø¨",
                "Ø§Ù„Ù…ÙƒØ§Ù† Ù†Ø¸ÙŠÙ", "Ù…ÙˆØ¸ÙÙŠÙ† Ù…Ø­ØªØ±Ù…ÙŠÙ†", "Ø´ØºÙ„ Ù…Ø±ØªØ¨", "ÙˆØ§Ùˆ Ø­Ø¨ÙŠØª", "ÙŠØ³ØªØ§Ù‡Ù„ ÙƒÙ„ Ø±ÙŠØ§Ù„",
                "ØºØ§Ù„ÙŠ Ø¹Ù„Ù‰ ÙØ§Ø¶ÙŠ", "Ù‚Ø±Ù Ø§Ø³ØªØºÙØ± Ø§Ù„Ù„Ù‡", "Ø§Ø³ÙˆØ¡ Ù…Ø·Ø¹Ù…", "Ø®Ø¯Ù…Ø© Ø¨Ø·ÙŠØ¦Ø©", "Ù„Ø§ ÙŠØ¹ÙŠØ¯ Ø§Ù„ØªØ¬Ø±Ø¨Ø©"
            ] * 50, # Ù†ÙƒØ±Ø± Ø§Ù„Ø¬Ù…Ù„ 50 Ù…Ø±Ø© Ø¹Ø´Ø§Ù† ÙŠØµÙŠØ± Ø¹Ù†Ø¯Ù†Ø§ 1000 Ø¬Ù…Ù„Ø©
            'label': [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0] * 50
        }
        df = pd.DataFrame(extra_data)
    
    print(f"âœ… Ready with {len(df)} samples! | Ø¬Ø§Ù‡Ø² Ø¨Ù€ {len(df)} Ø¹ÙŠÙ†Ø©!")

except Exception:
    print("ğŸ“‚ Creating new dataset file... | Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù Ø¨ÙŠØ§Ù†Ø§Øª Ø¬Ø¯ÙŠØ¯...")
    # (Ù†ÙØ³ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù„ÙŠ ÙÙˆÙ‚ Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª)
    exit()

# 3. Clean the text data | ØªÙ†Ø¸ÙŠÙ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù†ØµÙˆØµ
print("ğŸ§¹ Cleaning text... | Ø¬Ø§Ø±ÙŠ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ...")
df['cleaned_text'] = df['text'].apply(clean_arabic_text)

# 4. Split data into Train and Test sets | ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ø®ØªØ¨Ø§Ø±
# 80% Training, 20% Testing | 80% Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ùˆ 20% Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±
X_train, X_test, y_train, y_test = train_test_split(
    df['cleaned_text'], df['label'], test_size=0.2, random_state=42
)

# 5. Convert text to numbers (Vectorization) | ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ù„Ø£Ø±Ù‚Ø§Ù…
vectorizer = TfidfVectorizer(max_features=5000) 
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# 6. Train the AI Model (Naive Bayes) | ØªØ¯Ø±ÙŠØ¨ Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ
print("ğŸ§  Training the model... | Ø¬Ø§Ø±ÙŠ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„...")
model = MultinomialNB()
model.fit(X_train_tfidf, y_train)

# 7. Evaluate Model Performance | ØªÙ‚ÙŠÙŠÙ… Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„
y_pred = model.predict(X_test_tfidf)
accuracy = accuracy_score(y_test, y_pred)

print(f"\n Final Result | Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©")
print(f"Accuracy: {accuracy * 100:.2f}% | Ø¯Ù‚Ø© Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„")